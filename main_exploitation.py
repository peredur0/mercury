#! /usr/bin/env python3
# coding: utf-8

"""
Phase 2: Exploitation (pour chaque document, pour tous les SPAM, HAM et pour tout les documents)
  Données statistiques:
    - Distribution de ZIPF
        Le mot le plus fréquent est 2 fois plus présent que le 2, 3 fois plus que le 3e etc
    - Distribution selon la taille des mots
        > Pour les textes aléatoires les mots les plus courts sont plus fréquent
    - Proportion d'HAPAX (la proportiond de mot n'ayant qu'une seule occurence doit être vers 50%)

    ==> Résultat avec ou sans analyse syntaxique (Standford ou nltk)
    ==> Système de scoring ELK pour les mails, sqlite pour les catégories

  Données sémantiques
    - Extraction des thèmes
"""

from databases import elastic_cmd
from databases.elastic_docker import secrets

if __name__ == '__main__':
    print("=== Phase 2 : ???? ===")

    # == Récupération des données
    es_cli = elastic_cmd.es_connect(secrets.serveur, (secrets.apiid, secrets.apikey), secrets.ca_cert)
    index = "test_import_all0"
    data = {}
    for cat in ['spam', 'ham']:
        print("-- Récupération des {}...".format(cat), end=' ')
        data[cat] = elastic_cmd.es_get_all(es_cli, index, sort={'hash': 'asc'}, query={'match': {'categorie': cat}})
        print('OK')


    exit(0)
